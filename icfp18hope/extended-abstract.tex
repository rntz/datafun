% TyDe wants sigplan & ``Extended abstract'' clearly in the title.
% HOPE doesn't seem to care.
% FHPC wants acmart, doesn't specify sigplan.

% https://icfp18.sigplan.org/track/tyde-2018#Call-for-Contributions
% https://icfp18.sigplan.org/track/hope-2018-papers#Call-for-Presentations
% https://icfp18.sigplan.org/track/FHPC-2018-papers#FHPC-2018-Call-for-Papers

\documentclass[sigplan,screen,dvipsnames]{acmart}
\settopmatter{printacmref=false}
\setcopyright{none}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

%\usepackage[scaled=1.025]{cochineal}\linespread{1.025}
%\usepackage[scaled=1.05]{cochineal}\linespread{1.05}
%\usepackage[scaled=1.02]{cochineal}


%% ---- Packages ----
\usepackage{amsmath,latexsym,stmaryrd}
\usepackage{mathpartir}
\usepackage[nameinlink]{cleveref}
\usepackage{mathtools}          % \dblcolon
\usepackage{anyfontsize}
\usepackage{lipsum}
\usepackage{listings}
\lstset{language=prolog, columns=fullflexible,
  basicstyle=\ttfamily, commentstyle=\color{Green}}

%% %% Avoid ``too many math alphabets'' error.
%% \newcommand\hmmax{0}
%% \newcommand\bmmax{0}
%% \usepackage{bm}


%% ---- Top matter ----
\title{Finding fixed points faster}
% The derivative of a fixed point is the fixed point of its derivative
% Finding fixed points faster by differentiation
% Deriving incremental recursive queries

\author{Michael Arntzenius}
\affiliation{University of Birmingham}
\email{daekharel@gmail.com}


%% ---- Commands ----
\newcommand{\todo}[1]{{\color{ACMPurple}#1}}
\newcommand{\hilited}{\color{Cyan}}

\newcommand{\naive}{na\"ive}
\newcommand{\Naive}{Na\"ive}

\newcommand{\D}{\Delta}
\newcommand{\bnfeq}{\dblcolon=}
\newcommand{\bnfcont}{}
\newcommand{\pipe}{~|~}
\newcommand{\x}{\times}
\newcommand{\fn}{\lambda}
\newcommand{\binder}{.~}
\newcommand{\bind}[1]{#1\binder}
\newcommand{\fnof}[1]{\fn\bind{#1}}
\newcommand{\setfor}[2]{\{#1 ~|~ #2\}}

\renewcommand{\d}{\delta}
\newcommand{\zero}{\mathbf{0}}
\newcommand{\kw}[1]{\textsf{\bfseries #1}}
\newcommand{\tlv}[1]{\textsf{#1}}
\newcommand{\var}[1]{\mathit{#1}}
\newcommand{\dee}[1]{\var{d#1}}
\newcommand{\subst}[1]{[#1]\,}

\newcommand{\mto}{\overset{+}{\to}}
\renewcommand{\mto}{\Rightarrow}
\newcommand{\tset}[1]{\{#1\}}
\newcommand{\tbool}{\textsf{bool}}

\newcommand{\eset}[1]{\{#1\}}
\newcommand{\ewhen}[1]{\kw{when}\,(#1)~}
\newcommand{\eif}[2]{\kw{if}~#1~\kw{then}~#2~\kw{else}~}
\newcommand{\efor}[1]{\bigcup(#1)~}
\newcommand{\elet}[1]{\kw{let}~#1~\kw{in}~}
\newcommand{\efix}{\kw{fix}~}



\begin{document}
\thispagestyle{plain}
\setlength\arraycolsep{2.5pt}

\begin{abstract}
  I propose to talk about work-in-progress on generalising the classic Datalog
  optimisation \emph{semi\naive{} evaluation} to the higher-order functional
  language Datafun.
\end{abstract}

\maketitle

\section{Introduction}

%% Logic programming has a powerful appeal: declare what counts as a solution, and
%% let the computer find it! Unfortunately, traditional logic programming systems
%% like Prolog have trouble going truly \emph{higher-order} in the way functional
%% programmers are used to. Seeking to have their cake and eat it, too, functional
%% programmers have learned to emulate logic programming using the effect of
%% \emph{nondeterminism}, usually implemented as backtracking.

%% However, backtracking search is often inefficient; logic programmers have
%% explored other useful strategies. Datalog~\citep{datalog} takes an extreme
%% approach, allowing only predicates with finite extent. This allows bottom-up
%% evaluation, which easily handles queries (such as transitive closure) which are
%% difficult or inefficient to implement in Prolog.

Functional programmers have learned to emulate logic programming using the
effect of \emph{nondeterminism}, usually implemented as backtracking. However,
backtracking search is often inefficient; logic programmers have explored other
useful strategies.
%
Datalog~\citep{datalog} takes an extreme approach, allowing only predicates with
finite extent. This allows bottom-up evaluation, which easily handles queries
(such as transitive closure) that are inefficient to solve by brute-force
search.

Datafun~\cite{datafun} shows that higher-order functional programs can emulate
Datalog using a \emph{bottom-up nondeterminism effect} (a \emph{finite set
  monad}) combined with \emph{monotone fixed points}. Here, we sketch the
translation to Datafun of a classic Datalog optimisation, semi\naive{}
evaluation, which avoids needlessly re-deducing facts when evaluating a
recursive predicate.

Datalog folklore suggests semi\naive{} evaluation can be understood in terms of
\emph{derivatives}~\citep{bancilhon85,bancilhon86}; we substantiate this by
showing that its analogue in Datafun can be defined by applying recent work by
\citet{incremental} on derivatives for incremental computation in a higher-order
setting.


\section{Datalog, \naive{}ly and semi\naive{}ly}\label{sec:datalog}

This simple Datalog program computes reachability in a graph, given its
\texttt{edge} relation:
%
\begin{lstlisting}
path(X,Y) :- edge(X,Y).
path(X,Z) :- edge(X,Y), path(Y,Z).
\end{lstlisting}

A path is either an edge, or an edge followed by a path. But how does Datalog
find these paths? Let's identify a predicate with the set of argument-tuples it
holds of. Then we can compute \texttt{path} as the least fixed point of this
function:
%
\[
\begin{array}{l}
  \tlv{step}~ \var{path} = \setfor{(x,y)}{(x,y) \in \tlv{edge}}\\
  \phantom{\tlv{step}~ \var{path}} \hspace{.5pt}\cup
  \setfor{(x,z)}{(x,y) \in \tlv{edge}, (y,z) \in \var{path}}
\end{array}
\]

The \naive{} approach is to iterate the \tlv{step} function, computing the
sequence $\emptyset, \tlv{step}^1(\emptyset), \tlv{step}^2(\emptyset), ...$
until it reaches a fixed point $\tlv{step}^k(\emptyset) =
\tlv{step}^{k+1}(\emptyset)$.
%
This works, but observe that $\tlv{step}^i(\emptyset) \subseteq
\tlv{step}^{i+1}(\emptyset)$. This means we are doing \emph{redundant
  computation} --- if step $i$ appends an edge $(x,y)$ to a path $(y,z)$ to
discover the path $(x,z)$, step $i+1$ re-discovers it the same way. We really
want to compute the \emph{change} between iterations:
%
\[
\begin{array}{rcl}
  \tlv{path} &=& \tlv{iterate} \;\emptyset \;\tlv{edge}\\
  \tlv{iterate} \;x \;\dee x &=&
  \eif{\dee x \subseteq x}{x}\\
  && \tlv{loop} \;(x \cup \dee x) \;(\delta\tlv{step} \;dx)\\
  \delta\tlv{step} \;\var{dpaths} &=&
  \setfor{(x,z)}{(x,y) \in \tlv{edge}, (y,z) \in \var{dpaths}}\\
\end{array}
\]

In Datalog, it's long been known how to safely approximate this change using a
static transformation on Datalog rules (we omit its definition for space
reasons); this is known as \emph{semi\naive{}
  evaluation}~\citep{bancilhon85,bancilhon86}.


\section{Datafun, na\"ively}

%% TODO: omit if, when, etc?
%% or expand to include equality?

\begin{figure}
  \[
  \begin{array}{rcl}
    A,B &\bnfeq& \tbool \pipe \tset{A} \pipe A \to B \pipe A \mto B
    \\
    e &\bnfeq& \fnof{x} e \pipe e_1\:e_2 \pipe \eset{\vec{e}} \pipe e_1 \cup e_2
    \pipe \efor{x \in e_1}~ e_2
    \\ &\bnfcont&  \efix e \pipe \ewhen{e_1} e_2 \pipe \eif{e_1}{e_2}{e_3}
  \end{array}
  \]\vspace{-1em}
  \caption{A fragment of Datafun}
  \label{fig:datafun}
\end{figure}

You've actually already seen some Datafun code: the \tlv{step} function in
\cref{sec:datalog}! Datafun is a typed higher-order functional language equipped
with a \emph{finite set monad}, which supports set-comprehension syntax sugar in
the usual way~\cite{comprehending-monads}. Like Datalog, Datafun is
\emph{total}: all programs terminate.

\Cref{fig:datafun} gives the fragment of Datafun we consider here. For the full
language, see \citet{datafun}. We write monadic bind $\efor{x \in e_1} e_2$,
meaning ``the union of the sets $e_2$ for each $x \in e_1$''.
%
Datafun can also compute \emph{fixed points} of functions on finite sets, $\efix
f$.\footnote{The full language generalizes both monadic bind and fixed points to
  \emph{semilattice types}; for simplicity we here consider only finite sets.}

To ensure $\efix f$ terminates, $f$ must be (among other things) monotone ($x
\subseteq y$ implies $f\:x \subseteq f\:y$), so Datafun's type system tracks
monotonicity of functions and expressions.
%
The type $A \mto B$ represents monotone functions, a subtype of all functions $A
\to B$. The expression $\ewhen{e_1}{e_2}$ yields the set $e_2$ if $e_1$ is true,
and $\emptyset$ otherwise; unlike \kw{if}, this is always monotone in $e_1$.

As we've seen, Datalog programs can be expressed using a combination of set
operations and fixed points. For example, \texttt{path} is
$(\efix{\tlv{step}})$. However, the semi\naive{} evaluation transformation,
formulated on Datalog, does not handle higher-order functions. Can we lift this limitation?

\section{Derivatives for Datafun}

\Naive{} evaluation iterates a function $f$.
%
Semi\naive{} evaluation approximates the \emph{change} between iterations ---
how does $f(x)$ change as $x$ goes from $f^{i}(\emptyset)$ to
$f^{i+1}(\emptyset)$?
%
To answer this question for Datafun, we build on the \emph{incremental
  $\lambda$-calculus} of \citet{incremental}, which shows how to compute the
change to a function's result given a change to its input.
%
Here we summarize their approach and how we apply it to Datafun.

%% provides a static transformation assigning to every definable function $f : A
%% \to B$ a derivative $\d\!f : A \to \D A \to \D B$ such that $\d\!f \;x \;\dee x$
%% computes the change to $f\;x$ under a change $\dee x$ to its input.

To capture what change means, we assign to every type $A$ a \emph{change
  structure} $(\D A, \oplus, \zero)$.\footnote{\Citeauthor{incremental} also
  include an operator $\ominus$ from which $\zero$ is derived; our restriction
  to just $\zero$ is suggested by \citet{atkey-changes}.}
%
The type $\D A$ represents changes to values of type $A$. Since the steps of a fixed point computation increase monotonically, we need only represent \emph{increasing changes}.
%
The function $\oplus : A \to \Delta A \to A$ applies a change to a value.
Finally, $\zero : A \to \Delta A$ gives a \emph{zero change} such that $x \oplus
\zero\,x = x$.

\begin{figure}
  \[\begin{array}{rcl}
    \D\tbool &=& \tbool\\
    \D\tset{A} &=& \tset{A}\\
    \D(A \to B) &=& A \to \D A \to \D B\\
    \D(A \mto B) &=& A \to \D A \mto \D B
    \vspace{0.5em}\\
    \d x &=& dx\\
    \d(\fnof x e) &=& \fnof x \fnof{\dee x} \d{e}\\
    \d(e_1~e_2) &=& \d{e_1}~e_2~\d{e_2}\\
    \d{\eset{\vec e}} &=& \emptyset\\
    \d(e_1 \cup e_2) &=& \d{e_1} \cup \d{e_2}\\
    \d(\efor{x \in e_1} e_2)
    &=& \efor{x \in \d{e_1}} e_2\\
    &\cup& \efor{x \in e_1 \cup \d{e_1}}
    %\elet{\dee x = \dzero x}
    \subst{\zero\, x / \dee x}
    \d{e_2}
    \\
    \d(\efix f) &=& \efix (\d\!f~(\efix f))\\
    \d(\eif{e}{e_1}{e_2}) &=& \eif{e}{\d{e_1}}{\d{e_2}}\\
    \d(\ewhen{e_1}{e_2})
    &=& \eif{e_1}{\d{e_2}}\\
    && \ewhen{\d{e_1}} {e_2 \cup \d{e_2}}
  \end{array}\]
  \vspace{-.8em}
  \caption{Derivatives for a fragment of Datafun}
  \label{fig:derivatives}
\end{figure}

In \cref{fig:derivatives} we give a transformation from an expression $e : A$ to
its derivative $\d e : \D A$, which computes how $e$ changes as its free
variables change. By convention, the change to a variable $x_i : A_i$ is given
by a variable named $\dee x_i : \D A_i$.

For our purpose, the most important change structures are those on finite sets
and on functions. Sets are ordered by inclusion, so increasing a set means is
simply it with a set of added elements. Thus $\D\tset{A} = \tset{A}$, $\oplus =
\cup$, and $\zero\,x = \emptyset$.

Putting monotonicity aside, the change type for functions is $\D(A \to B) = A
\to \D A \to \D B$. Why not simply $A \to \D B$? Because in the derivative of
function application $\d(e_1\;e_2)$, it isn't only the function that may change,
but its argument!

%% TODO: talk more about function changes!

\subsection{How to compute fixed points faster}

We promised an analogue of semi\naive{} evaluation --- a way to find fixed
points faster. How do derivatives help us? Well, given $\efix f$, the expression
$\d\!f \;x \;dx$ tells us how $f$ changes as its argument $x$ changes to $x \cup
dx$. This is exactly what we need: to compute the change between steps in our
fixed point iteration. We give the na\"ive and semina\"ive algorithms for computing fixed points in \cref{fig:defining-fix}.

\begin{figure}
  \[
  \begin{array}{rcl}
  \efix f &=& \tlv{\naive}_f \,\emptyset \,(f\, \emptyset)\\
  \tlv{\naive}_f \,x \,\var{next}
  &=& \kw{if}~ x = \var{next} ~\kw{then}~ x ~\kw{else}\\
  && \tlv{\naive}_f \,\var{next} \,(f \,\var{next})

  \vspace{1em}\\

  \efix f &=& \tlv{semi\naive}_f \,\emptyset \,(f\, \emptyset)\\
  \tlv{semi\naive}_f \,x \,\dee x
  &=& \kw{if}~ \dee x \subseteq x ~\kw{then}~ x ~\kw{else}\\
  && \tlv{semi\naive}_f \,(x \cup \dee x) \,(\d\!f \,x \,\dee x)
  \end{array}
  \]
  \vspace{-.5em}
  \caption{Na\"ive and semina\"ive fixed point computation}
  \label{fig:defining-fix}
\end{figure}

\subsection{Defining \texorpdfstring{$\d(\efix f)$}{d(fix f)}}

The definition of $\d(\efix f)$ in \cref{fig:derivatives} may seem a little
mysterious. However, it's easy to show that $\d(\efix f)$ must be a fixed point
of $\d\!f\;(\efix f)$:
%% We discovered the definition of $\d(\efix f)$ in \cref{fig:derivatives} by
%% observing that, if it exists, $\d(\efix f)$ must be a fixed point of $\d\!f\;
%% (\efix f)$:
%
\begin{align*}
  {\hilited \d(\efix f)}
  &= \d(f\: (\efix f))
  & \text{expand fixed point}\\
  &= \d\!f\; (\efix f)\; {\hilited \d(\efix f)}
  & \text{rule for}~\delta(e_1\;e_2)
\end{align*}
%
Which suggests the following motto:
\begin{center}
  \large \scshape
  the derivative of a fixed point\nopagebreak\\
  is the fixed point of its derivative.
\end{center}
%
This is so beautiful it must be true. Nevertheless, we have proven it
correct~\citep{fixderiv}.


\section{Contributions}

Our main contributions are:%\vspace{1ex}

\begin{enumerate}\setlength\itemsep{1ex}
\item Generalising semi\naive{} evaluation to a higher-order functional
  language, Datafun, giving an optimisation for finding fixed points faster.

\item Substantiating folklore that semi\naive{} evaluation can be understood in
  terms of derivatives.

\item Extending the work of \citet{incremental} to handle a finite set monad,
  fixed points, and interaction with monotonicity.
\end{enumerate}


%% Bibliography
\bibliographystyle{abbrvnat}
\bibliography{datafun}

\end{document}
