\section{From Semi\naive{} Evaluation to the Incremental \boldfn-Calculus}
\label{sec:seminaive-and-ilc}

Consider the following Datalog fragment:\footnote{This example and explanation
  of semi\naive\ evaluation is borrowed almost entirely from
  \citet{DBLP:conf/esop/Alvarez-Picallo19}.}

\begin{align*}
  \name{path}(x,y) &\gets \name{edge}(x,y)
  &
  \name{path}(x,z) &\gets \name{edge}(x,y) \wedge \name{path}(y,z)
\end{align*}


\noindent
The denotation of \name{path} is the least fixed point of the rules defining
it.
% mention Kleene fixed point theorem?
We can compute this by repeatedly applying these rules until the collection
of known paths (initially empty) stops growing.
%
For example, if the \name{edge} relation is \{(1,\,2), (2,\,3), (3,\,4)\}, we
get the following evaluation trace:

%% TODO: make sure there's enough vertical space around this.
\begin{center}
  \setlength\tabcolsep{1em}
  \begin{tabular}{@{}rll@{}}
    Step
    & Previously known paths
    & Paths deduced at this step
    \\\midrule
    0
    & none
    & (1,\,2) (2,\,3) (3,\,4)
    \\
    1
    & (1,\,2) (2,\,3) (3,\,4)
    & (1,\,2) (2,\,3) (3,\,4) (1,\,3) (2,\,4)
    \\
    2
    & (1,\,2) (2,\,3) (3,\,4) (1,\,3) (2,\,4)
    & (1,\,2) (2,\,3) (3,\,4) (1,\,3) (2,\,4) (1,\,4)
    \\
    3
    & (1,\,2) (2,\,3) (3,\,4) (1,\,3) (2,\,4) (1,\,4)
    & as above
  \end{tabular}
\end{center}

\noindent We have now reached the desired fixed point. However, this process is
quite wasteful: we deduced the path (1,\,2) at \emph{every} iteration; ideally
we'd only deduce it once. On a chain of $n$ edges, we deduce $\Theta(n^3)$
facts, even though there are only $\Theta(n^2)$ paths!

The standard improvement to this strategy is
\emph{semi\naive\ evaluation}~\cite{seminaive}, which transforms recursive rules
into explicitly iterative time-indexed rules of two kinds: \emph{derivative}
rules, to compute the new facts at each iteration; and \emph{accumulator} rules,
to collect these facts into a final result.
%
In this case, the derivative rule is simple: we discover new paths at iteration
$i+1$ by appending edges to paths which were new at iteration $i$:

\begin{align*}
  \name{dpath}_0(x,y) &\gets \name{edge}(x,y)
  \\
  \name{dpath}_{i+1}(x,z) &\gets \name{edge}(x,y) \wedge \name{dpath}_i(y,z)
  \\
  \name{path}_{i+1}(x,y) &\gets \name{path}_i(x,y) \vee \name{dpath}_i(x,y)
\end{align*}

\noindent This yields the execution trace:

\begin{center}
  \setlength\tabcolsep{1em}
  \begin{tabular}{@{}rll@{}}
    Step & $\name{path}_i$ & $\name{dpath}_i$
    \\\midrule
    0 & empty & (1,\,2) (2,\,3) (3,\,4)
    \\
    1 & (1,\,2) (2,\,3) (3,\,4) & (1,\,3) (2,\,4)
    \\
    2 & (1,\,2) (2,\,3) (3,\,4) (1,\,3) (2,\,4) & (1,\,4)
    \\
    3 & (1,\,2) (2,\,3) (3,\,4) (1,\,3) (2,\,4) (1,\,4) & empty
  \end{tabular}
\end{center}

%% TODO: email Alvarez-Picallo, Michael P-J, etc; they say "a quadratic
%% computation into a linear one" but AFAICT this is not so! Unless the claim is
%% that the naive version is quadratic in the _output_?!
%%
%% Perhaps we should say, "--- our computation is now linear in the size of its
%% output". But is this really justified?
\noindent
This is much better --- we have turned a cubic computation into a quadratic one!


\subsection{Semi\naive\ evaluation as incremental computation}
\label{sec:seminaive-tc-in-datafun}

Now let's move from Datalog to Datafun. The transitive closure of \name{edge} is
the fixed point of the monotone function \name{step} defined by:

\[
\name{step} \<\name{path} = \name{edge} \cup
\setfor{(x,z)}{(x,y) \in \name{edge}, (y,z) \in \name{path}}
\]

\noindent
The \naive\ way to compute \name{step}'s fixed point is to iterate it: start
with \(\name{path}_0 = \emptyset\) and compute \(\name{path}_{i+1} =
\name{step}\<\name{path}_i\) for increasing $i$ until \(\name{path}_i =
\name{path}_{i+1}\).
%
But since $\name{path}_i \subseteq \name{step}\<\name{path}_i$, each iteration
re-computes every path found by the previous iteration.
%
Following Datalog, we'd prefer to compute only the \emph{change} between
iterations.
%
So consider $\name{step}'$ defined by:

\[
\name{step}' \<\name{dpath} =
\setfor{(x,z)}{(x,y) \in \name{edge}, (y,z) \in \name{dpath}}
\]

%% according to http://mkweb.bcgsc.ca/colorblind/ and Figure 2 in
%% https://www.nature.com/articles/nmeth.1618?WT.ec_id=NMETH-201106, Vermilion
%% and Blue make a reasonable contrasting color combo. However, both are a
%% little light for use as text on a white background so we add some black (K in
%% CMYK).
\definecolor{BlindVermilion}{cmyk}{0,0.8,1,0.1}
\definecolor{BlindBlue}{cmyk}{1,0.5,0,0.1}

\newcommand\colorpath{{\color{BlindVermilion}\name{path}}}
\newcommand\colordpath{{\color{BlindBlue}\name{dpath}}}
\renewcommand\colorpath{\name{path}}
\renewcommand\colordpath{\name{dpath}}

\noindent
Observe that
%
\begin{align*}
  &\mathrel{\hphantom{=}} {\color{BlindVermilion}\name{step}\<\colorpath}
  \cup
  {\color{BlindBlue}\name{step}'\<\colordpath}
  \\
  &=
  {\color{BlindVermilion}
  \name{edge} \cup \setfor{(x,z)}{(x,y) \in \name{edge}, (y,z) \in \colorpath}}
  \cup
  {\color{BlindBlue}
  \setfor{(x,z)}{(x,y) \in \name{edge}, (y,z) \in \colordpath}}
  \\
  &=
  \name{edge} \cup
  \setfor{(x,z)}{(x,y) \in \name{edge}, (y,z) \in \colorpath \cup \colordpath}
  \\
  &=
  \name{step} \<(\colorpath \cup \colordpath)
\end{align*}

\noindent
In other words, $\name{step}'$ tells us how \name{step} changes as its input
grows.
%
Using this property, we can directly compute the changes $\name{dpath}_i$
between our iterations $\name{path}_i$:

%% \begin{align*}
%%   \name{dpath}_0 &= \name{step}\<\emptyset
%%   & \name{dpath}_{i+1} &= \name{step}'\<\name{dpath}_i
%%   & \name{path}_{i+1} &= \name{path}_i \cup \name{dpath}_i
%% \end{align*}

%% \[\def\arraystretch{1.2}
%% \begin{array}{rclcl}
%%   \name{dpath}_0 &=& \name{step}\<\emptyset
%%   &=& \name{edge}
%%   \\
%%   \name{dpath}_{i+1} &=& \name{step}'\<\name{dpath}_i
%%   &=& \setfor{(x,z)}{(x,y) \in \name{edge}, (y,z) \in \name{dpath}_i}
%%   \\
%%   \name{path}_{i+1} &=& \name{path}_i \cup \name{dpath}_i
%% \end{array}\]

\begin{align*}
  \name{dpath}_0
  &= \name{step}\<\emptyset
  = \name{edge}
  \\
  \name{dpath}_{i+1}
  &= \name{step}'\<\name{dpath}_i
  = \setfor{(x,z)}{(x,y) \in \name{edge}, (y,z) \in \name{dpath}_i}
  \\
  \name{path}_{i+1}
  &= \name{path}_i \cup \name{dpath}_i
\end{align*}

\noindent These exactly mirror the derivative and accumulator rules for
\(\name{path}_i\) and \(\name{dpath}_i\) we gave earlier.
%
\todo{Explain how this lets us compute $\name{path}_i$ more
  efficiently and wait until it quiesces as before.}

The problem of semi\naive\ evaluation for Datafun, then, reduces to the problem
of finding functions, like $\name{step}'$, which compute the change in a
function's output given a change to its input.
%
This is a problem of \emph{incremental computation}, and since Datafun is a
functional language, we turn to the \emph{incremental
  \fn-calculus}~\citep{incremental,DBLP:conf/esop/GiarrussoRS19}.


\subsection{Change structures}
\label{sec:change-structures}

To make precise the notion of change, an incremental \fn-calculus associates
every type $A$ with a \emph{change structure}, consisting of:\footnote{Our
  notion of change structure differs significantly from that of
  \citet{incremental}, although it is similar to the logical relation given in
  \citet{DBLP:conf/esop/GiarrussoRS19}; we discuss this in
  \cref{sec:differences-from-incremental}. Although we do not use change
  structures \emph{per se} in our proofs, they are an important source of
  intuition.}

\begin{enumerate}
\item A type $\D A$ of possible changes to values of type $A$.
\item A relation $\changesat{A}{\dx}{x}{y}$ for $\dx : \D A$ and $x,y : A$,
  glossed as ``$\dx$ changes $x$ into $y$''.
\end{enumerate}

\noindent
Since the iterations of a fixed point grow monotonically, in Datafun we only
need \emph{increasing} changes.
%
For example, sets change by gaining new elements:

\begin{align*}
  \D\tseteq{A} &= \tseteq{A}
  &
  \changesat{\tseteq{A}}{\dx}{x}{x \cup \dx}
\end{align*}

Set changes may be the most significant for fixed point purposes, but to handle
all of Datafun we need a change structure for every type. For products and sums,
for example, the change structure is pointwise:

%% \begin{center}
%%   \setlength\tabcolsep{10pt}
%%   \begin{tabular}{@{}ccc@{}}
%%     $\D\tunit = \tunit$
%%     &
%%     \(\D(A \x B) = \D A \x \D B\)
%%     &
%%     \(\D(A + B) = \D A + \D B\)
%%     \\[8pt]
%%     \(\changesat{\tunit}{\tuple{}}{\tuple{}}{\tuple{}}\)
%%     &
%%     \(\infer{
%%       \changesat{A}{\da}{a_1}{a_2}
%%       \\
%%       \changesat{B}{\db}{b_1}{b_2}
%%     }{\changesat{A \x B}
%%       {\tuple{\da,\db}}
%%       {\tuple{a_1,b_1}}
%%       {\tuple{a_2,b_2}}
%%     }\)
%%     &
%%     \(\infer{
%%       \changesat{A_i}{\dx_i}{x_i}{y_i}
%%     }{
%%       \changesat{A_1 + A_2}{\inj i \dx}{\inj i x}{\inj i y}
%%     }\)
%%   \end{tabular}
%% \end{center}

\begin{align*}
  \D\tunit &= \tunit
  &
  \D(A \x B) &= \D A \x \D B
  &
  \D(A + B) &= \D A + \D B
\end{align*}

\begin{align*}
  \changesat{\tunit}{\tuple{}}{\tuple{}}{\tuple{}}
  &&
  %% \infer{
  %%   \fa{i} \changesat{A_i}{\dx_i}{x_i}{y_i}
  %% }{\changesat{A_1 \x A_2}
  %%   {\tuple{\vec\dx}}
  %%   {\tuple{\vec x}}
  %%   {\tuple{\vec y}}
  %% }
  %
  %% \infer{
  %%   \fa{i} \changesat{A_i}{\dx_i}{x_i}{y_i}
  %% }{\changesat{A_1 \x A_2}
  %%   {\tuple{\dx_1,\dx_2}}
  %%   {\tuple{x_1,x_2}}
  %%   {\tuple{y_1,y_2}}
  %% }
  %
  \infer{
    \changesat{A}{\da}{a}{a'}
    \\
    \changesat{B}{\db}{b}{b'}
  }{\changesat{A \x B}
    {\tuple{\da,\db}}
    {\tuple{a,b}}
    {\tuple{a',b'}}
  }
  &&
  \infer{
    \changesat{A_i}{\dx}{x}{y}
  }{
    \changesat{A_1 + A_2}{\inj i \dx}{\inj i x}{\inj i y}
  }
\end{align*}
%\vspace{0pt} % TODO: double-check if this is better.

Since we only consider increasing changes, and $\iso A$ is ordered discretely,
the only ``change'' permitted is to stay the same. Consequently, no information
is necessary to indicate what changed:

\begin{align*}
  \D(\iso A) &= \tunit
  &&
  \changesat{\iso A}{\tuple{}}{x}{x}
\end{align*}

Finally we come to the most interesting case: functions.

\begin{align*}
  \D(A \to B) &= \iso A \to \D A \to \D B
  &
  \infer[FnChange]{
    \fa{\changesat A \dx x y}
    \changesat B {\df\<x\<\dx} {f\<x} {g\<y}
  }{
    \changesat{A \to B}{\df}{f}{g}
  }
\end{align*}

\noindent
Observe that a function change $\df$ takes two
arguments: a base point $x : \iso A$ and a change $\dx : \D A$.
%
To understand why we need both, consider incrementalizing function application:
we wish to know how $f\<x$ changes as both $f$ and $x$ change. So fix
$\changes{\df}{f}{g}$ and $\changes{\dx}{x}{y}$. How do we find the change $f\<x
\changesto g\<y$ that updates both function and argument?

If changes were given
pointwise, taking only a base point, we'd stipulate that $\changes{\df}{f} g$
iff $\fa{x} \changes{\df\<x}{f\<x}{g\<x}$. But this only gets us to $g\<x$, not
$g\<y$: we've accounted for the change in the function, but not the argument.
%
We can account for both by giving $\df$ an additional parameter: not just the
base point $x$ but also the change $\dx$ to it. Then by inverting
\textsc{FnChange} we have $\changes{\df\<x\<\dx}{f\<x}{g\<y}$ as desired.

%% This makes it easy to incrementalize function application, $f\<x$; given
%% changes $\changes \df f g$ and $\changes \dx x y$ to the function and its
%% argument, we want to compute the change that takes us to the updated
%% application $g\<y$. By inverting \textsc{FnChange} we know that
%% $\changes{\df\<x\<\dx}{f\<x}{g\<y}$, so $\df\<x\<\dx$ gives us the desired
%% change.

%% If instead changes were given pointwise, letting $\D(A \to B)= \iso A \to \D B$,
%% then it'd be natural to let $\changes{\df}{f}{g} \iff \fa{x}
%% \changes{\df\<x}{f\<x}{g\<x}$.

Note also the mixture of monotonicity and non-monotonicity in the type $\iso A
\to \D A \to \D B$. Since our functions are monotone --- increasing inputs yield
increasing outputs --- function \emph{changes} are also monotone on input
changes $\D A$ --- a larger increase in the input yields a larger increase in
the output. However, there's no reason to expect the change in the output to
grow as the \emph{base point} increases --- hence the use of $\iso$.


\subsection{Zero-changes, derivatives, and faster fixed points}
\label{sec:derivatives}

If $\changesat A \dx x x$, we call \dx\ a \emph{zero-change} to $x$. Usually
zero-changes are rather boring --- for example, a zero change to a set $x :
\tseteq{A}$ is any $\dx \subseteq x$, and so $\emptyset$ is always a zero
change.
%
However, there is one very interesting exception: function zero changes. Suppose
$\changesat{A \to B}{\df}{f}{f}$. This implies that

\begin{equation}\label{eqn:derivative}
  \changesat A \dx x y \implies \changesat B{\df\<x\<\dx}{f\<x}{f\<y}
\end{equation}

\noindent
In other words, $\df$ yields the change in the output of $f$ given a change to
its input.
%
This is exactly the property of $\name{step}'$ that made it useful for
semi\naive\ evaluation --- indeed, $\name{step}'$ is a zero-change to
\name{step}, modulo not taking the base point $x$ as an argument:

\[ \changesat{\tseteq A} \dx x y \implies
\changesat{\tseteq A}{\name{step}'\<\dx}{\name{step}\<x}{\name{step}\<y}
\]

\noindent
Function zero-changes are so important we give them a special name:
\emph{derivatives}. We now have enough machinery to prove correct a general
\emph{semi\naive\ fixed point strategy}. First, observe that:

\begin{lemma}\label{lem:DeltaL}
  At every semilattice type $L$, we have $\D L = L$ and
  $\changesat{L}{\dx}{x}{y} \iff (x \binvee \dx) = y$.
\end{lemma}
\begin{proof}
  By induction on semilattice types $L$. \todo{See appendix.}
\end{proof}

\noindent
Now, given a monotone map $f : L \to L$ and its derivative $f' : \iso L \to L
\to L$, we can find $f$'s fixed-point as the limit of the sequence $x_i$
defined:

\begin{align*}
  x_0 &= \bot & x_{i+1} &= x_i \vee \dx_i\\
  \dx_0 &= f\<\bot & \dx_{i+1} &= f'\<x_i\<\dx_i
\end{align*}

\noindent Let $\fastfix\<(f,f') = \bigvee_i x_i$. By induction and the
derivative property, we have $\changes{\dx_{i+1}}{x_i}{f\<x_i}$ and so $x_i =
f^i\<x$, and therefore $\fastfix\<(f,f') = \efix f$. \todo{Moreover, if $L$
  satisfies the ascending chain property, we will reach our fixed point $x_i =
  x_{i+1}$ in a finite number of iterations.}

\label{sec:seminaive-strategy}

This leads directly to our strategy for semi\naive\ Datafun. \Citet{incremental}
defines a static transformation $\Deriv e$ which computes the change in $e$
given the change in its free variables; it \emph{incrementalizes} $e$. Our goal
is not to incrementalize Datafun \emph{per se}, but to find fixed points faster.
Consequently, we define two mutually recursive transformations: $\phi e$, which
computes $e$ faster by replacing fixed points with calls to \fastfix; and
$\delta e$, which incrementalizes $\phi e$ so that we can compute the derivative
of fixed point functions.
%
In order to define $\phi$ and $\delta$ and show them correct, however, we first
need a fuller account of Datafun's type system and semantics.


%% \subsection{Examples of semi\naive\ Datafun programs}
%% \XXX
