\section{From Semi\naive{} Evaluation to the Incremental \boldfn-Calculus}
\label{sec:seminaive-and-ilc}

Let's return to our example Datalog program, modified to consider graphs rather
than ancestry:

\begin{minted}{prolog}
  path(X, Z) ← edge(X, Z).
  path(X, Z) ← edge(X,Y), path(Y, Z).
\end{minted}

\noindent
Let's suppose that \rel{edge} denotes a linear graph, $\{(a_1, a_2),\, (a_2,
a_3),\, \dots,\, (a_{n-1}, a_n)\}$. Then \rel{path} should denote its
reachability relation, $\setfor{(a_i, a_j)}{1 \le i < j \le n}$. How can we
compute this relation? The \naive\ approach is to begin with nothing in the
\rel{path} relation and repeatedly apply its rules until nothing more is
deducible. We can make this precise by explicitly time-indexing our rules:

\begin{align*}
  \name{path}_{i+1}(X,Z) &\gets \name{edge}(X,Z)
  &
  \name{path}_{i+1}(X,Z) &\gets \name{path}_i(X,Y) \wedge \name{edge}(Y,Z)
\end{align*}

By omission $\name{path}_0 = \emptyset$. From this it's easy to see that
inductively $\name{path}_i \subseteq \name{path}_{i+1}$.
%
Consequently, at step $i+1$ we re-deduce every fact known at step $i$.
%
For example, suppose $\name{path}_i(a_j, a_k)$. Then at step $i+1$ we apply the
second rule to $\name{edge}(a_{j-1}, a_j)$ and deduce
$\name{path}_{i+1}(a_{j-1}, a_k)$.
%
But since we also have $\name{path}_{i+1}(a_j, a_k)$, at time $i+2$ we deduce
the very same thing again, and again at $i+3$, $i+4$, and so on.

Because we append edges one at a time, $\name{path}_i$ contains exactly paths of
$i$ or fewer edges. Therefore it takes $n$ steps until we reach our fixed point
$\name{path}_{n-1} = \name{path}_n$. Since step $i$ involves $|\name{path}_i|
\in \Theta(i^2)$ deductions, we make $\Theta(n^3)$ deductions in total. This
seems wasteful, since there are only $\Theta(n^2)$ paths in the final result.

Semi\naive\ evaluation avoids this waste by transforming the rules for
\rel{path} to find the newly deducible paths, $\name{dpath}_i$, at iteration
$i$, and accumulating these changes to produce a final result:

\begin{align*}
  \name{dpath}_0(X,Y) &\gets \name{edge}(X,Y)
  \\
  \name{dpath}_{i+1}(X,Z) &\gets \name{edge}(X,Y) \wedge \name{dpath}_i(Y,Z)
  \\
  \name{path}_{i+1}(X,Y) &\gets \name{path}_i(X,Y) \vee\name{dpath}_i(X,Y)
\end{align*}

\noindent
It's easy to show inductively that $\name{dpath}_{i}$ contains only paths
\emph{exactly} $i+1$ edges long. Consequently $|\name{dpath}_i| \in \Theta(n-i)$
and we make $\Theta(n^2)$ deductions overall.\footnote{Here we must assume the
  accumulation rule $\name{path}_{i+1}(X,Y) \gets \name{path}_i(X,Y) \vee
  \name{dpath}_i(X,Y)$ is implemented using an union operator that is efficient
  when the sets being unioned are of greatly differing sizes.}


\subsection{Semi\naive\ evaluation as incremental computation}
\label{sec:seminaive-tc-in-datafun}

Now let's move from Datalog to Datafun.\footnote{In this section we do not bother marking discrete variables $\dvar x$ or expressions $\eiso e$, as it muddies our examples to no benefit.} The transitive closure of \name{edge} is
the fixed point of the monotone function \name{step} defined by:

\[
\name{step} \<\name{path} = \name{edge} \cup
\setfor{(x,z)}{(x,y) \in \name{edge}, (y,z) \in \name{path}}
\]

\noindent
The \naive\ way to compute \name{step}'s fixed point is to iterate it: start
with \(\name{path}_0 = \emptyset\) and compute \(\name{path}_{i+1} =
\name{step}\<\name{path}_i\) for increasing $i$ until \(\name{path}_i =
\name{path}_{i+1}\).
%
But since $\name{path}_i \subseteq \name{step}\<\name{path}_i$, each iteration
re-computes every path found by the previous iteration.
%
Following Datalog, we'd prefer to compute only the \emph{change} between
iterations.
%
So consider $\name{step}'$ defined by:

\[
\name{step}' \<\name{dpath} =
\setfor{(x,z)}{(x,y) \in \name{edge}, (y,z) \in \name{dpath}}
\]

\newcommand\colorpath{{\color{BlindVermilion}\name{path}}}
\newcommand\colordpath{{\color{BlindBlue}\name{dpath}}}
\newcommand\colorA{\color{ColorA}}
\newcommand\colorB{\color{ColorB}}
\renewcommand\colorpath{\name{path}}
\renewcommand\colordpath{\name{dpath}}
%% \renewcommand\colorA\relax
%% \renewcommand\colorB\relax

\noindent
Observe that
%
\begin{align*}
  &\mathrel{\hphantom{=}} \name{step} \<(\colorpath \cup \colordpath)
  \\
  &= \name{edge} \cup \setfor{(x,z)}{(x,y) \in \name{edge}, (y,z) \in \colorpath \cup \colordpath}
  \\
  &= {\colorA \name{edge} \cup \setfor{(x,z)}{(x,y) \in \name{edge}, (y,z) \in \colorpath}} \cup {\colorB \setfor{(x,z)}{(x,y) \in \name{edge}, (y,z) \in \colordpath}}
  \\
  &= {\colorA\name{step}\<\colorpath} \cup {\colorB\name{step}'\<\colordpath}
\end{align*}

\noindent
In other words, $\name{step}'$ tells us how \name{step} changes as its input
grows.
%
Using this property, we can directly compute the changes $\name{dpath}_i$
between our iterations $\name{path}_i$:

%% \begin{align*}
%%   \name{dpath}_0 &= \name{step}\<\emptyset
%%   & \name{dpath}_{i+1} &= \name{step}'\<\name{dpath}_i
%%   & \name{path}_{i+1} &= \name{path}_i \cup \name{dpath}_i
%% \end{align*}

%% \[\def\arraystretch{1.2}
%% \begin{array}{rclcl}
%%   \name{dpath}_0 &=& \name{step}\<\emptyset
%%   &=& \name{edge}
%%   \\
%%   \name{dpath}_{i+1} &=& \name{step}'\<\name{dpath}_i
%%   &=& \setfor{(x,z)}{(x,y) \in \name{edge}, (y,z) \in \name{dpath}_i}
%%   \\
%%   \name{path}_{i+1} &=& \name{path}_i \cup \name{dpath}_i
%% \end{array}\]

\begin{align*}
  \name{dpath}_0
  &= \name{step}\<\emptyset
  = \name{edge}
  \\
  \name{dpath}_{i+1}
  &= \name{step}'\<\name{dpath}_i
  = \setfor{(x,z)}{(x,y) \in \name{edge}, (y,z) \in \name{dpath}_i}
  \\
  \name{path}_{i+1}
  &= \name{path}_i \cup \name{dpath}_i
\end{align*}

\noindent These exactly mirror the derivative and accumulator rules for
\(\name{path}_i\) and \(\name{dpath}_i\) we gave earlier.

%% TODO: Explain how this lets us compute $\name{path}_i$ more
%% efficiently and wait until it quiesces as before.

The problem of semi\naive\ evaluation for Datafun, then, reduces to the problem
of finding functions, like $\name{step}'$, which compute the change in a
function's output given a change to its input.
%
This is a problem of \emph{incremental computation}, and since Datafun is a
functional language, we turn to the \emph{incremental
  \fn-calculus}~\citep{incremental,DBLP:conf/esop/GiarrussoRS19}.


\subsection{Change structures}
\label{sec:change-structures}

To make precise the notion of change, an incremental \fn-calculus associates
every type $A$ with a \emph{change structure}, consisting of:\footnote{Our
  notion of change structure differs significantly from that of
  \citet{incremental}, although it is similar to the logical relation given in
  \citet{DBLP:conf/esop/GiarrussoRS19}; we discuss this in
  \cref{sec:differences-from-incremental}. Although we do not use change
  structures \emph{per se} in our proofs, they are an important source of
  intuition.}

\begin{enumerate}
\item A type $\D A$ of possible changes to values of type $A$.
\item A relation $\changesat{A}{\dx}{x}{y}$ for $\dx : \D A$ and $x,y : A$,
  glossed as ``$\dx$ changes $x$ into $y$''.
\end{enumerate}

\noindent
Since the iterations of a fixed point grow monotonically, in Datafun we only
need \emph{increasing} changes.
%
For example, sets change by gaining new elements:

\begin{align*}
  \D\tseteq{A} &= \tseteq{A}
  &
  \changesat{\tseteq{A}}{\dx}{x}{x \cup \dx}
\end{align*}

Set changes may be the most significant for fixed point purposes, but to handle
all of Datafun we need a change structure for every type. For products and sums,
for example, the change structure is pointwise:

%% \begin{center}
%%   \setlength\tabcolsep{10pt}
%%   \begin{tabular}{@{}ccc@{}}
%%     $\D\tunit = \tunit$
%%     &
%%     \(\D(A \x B) = \D A \x \D B\)
%%     &
%%     \(\D(A + B) = \D A + \D B\)
%%     \\[8pt]
%%     \(\changesat{\tunit}{\tuple{}}{\tuple{}}{\tuple{}}\)
%%     &
%%     \(\infer{
%%       \changesat{A}{\da}{a_1}{a_2}
%%       \\
%%       \changesat{B}{\db}{b_1}{b_2}
%%     }{\changesat{A \x B}
%%       {\tuple{\da,\db}}
%%       {\tuple{a_1,b_1}}
%%       {\tuple{a_2,b_2}}
%%     }\)
%%     &
%%     \(\infer{
%%       \changesat{A_i}{\dx_i}{x_i}{y_i}
%%     }{
%%       \changesat{A_1 + A_2}{\inj i \dx}{\inj i x}{\inj i y}
%%     }\)
%%   \end{tabular}
%% \end{center}

\begin{align*}
  \D\tunit &= \tunit
  &
  \D(A \x B) &= \D A \x \D B
  &
  \D(A + B) &= \D A + \D B
\end{align*}

\begin{align*}
  \changesat{\tunit}{\tuple{}}{\tuple{}}{\tuple{}}
  &&
  %% \infer{
  %%   \fa{i} \changesat{A_i}{\dx_i}{x_i}{y_i}
  %% }{\changesat{A_1 \x A_2}
  %%   {\tuple{\vec\dx}}
  %%   {\tuple{\vec x}}
  %%   {\tuple{\vec y}}
  %% }
  %
  %% \infer{
  %%   \fa{i} \changesat{A_i}{\dx_i}{x_i}{y_i}
  %% }{\changesat{A_1 \x A_2}
  %%   {\tuple{\dx_1,\dx_2}}
  %%   {\tuple{x_1,x_2}}
  %%   {\tuple{y_1,y_2}}
  %% }
  %
  \infer{
    \changesat{A}{\da}{a}{a'}
    \\
    \changesat{B}{\db}{b}{b'}
  }{\changesat{A \x B}
    {\tuple{\da,\db}}
    {\tuple{a,b}}
    {\tuple{a',b'}}
  }
  &&
  \infer{
    \changesat{A_i}{\dx}{x}{y}
  }{
    \changesat{A_1 + A_2}{\inj i \dx}{\inj i x}{\inj i y}
  }
\end{align*}
%\vspace{0pt} % TODO: double-check if this is better.

Since we only consider increasing changes, and $\iso A$ is ordered discretely,
the only ``change'' permitted is to stay the same. Consequently, no information
is necessary to indicate what changed:

\begin{align*}
  \D(\iso A) &= \tunit
  &&
  \changesat{\iso A}{\tuple{}}{x}{x}
\end{align*}

Finally we come to the most interesting case: functions.

\begin{align*}
  \D(A \to B) &= \iso A \to \D A \to \D B
  &
  \infer[FnChange]{
    \fa{\changesat A \dx x y}
    \changesat B {\df\<x\<\dx} {f\<x} {g\<y}
  }{
    \changesat{A \to B}{\df}{f}{g}
  }
\end{align*}

\noindent
Observe that a function change $\df$ takes two
arguments: a base point $x : \iso A$ and a change $\dx : \D A$.
%
To understand why we need both, consider incrementalizing function application:
we wish to know how $f\<x$ changes as both $f$ and $x$ change. So fix
$\changes{\df}{f}{g}$ and $\changes{\dx}{x}{y}$. How do we find the change $f\<x
\changesto g\<y$ that updates both function and argument?

If changes were given
pointwise, taking only a base point, we'd stipulate that $\changes{\df}{f} g$
iff $\fa{x} \changes{\df\<x}{f\<x}{g\<x}$. But this only gets us to $g\<x$, not
$g\<y$: we've accounted for the change in the function, but not the argument.
%
We can account for both by giving $\df$ an additional parameter: not just the
base point $x$ but also the change $\dx$ to it. Then by inverting
\textsc{FnChange} we have $\changes{\df\<x\<\dx}{f\<x}{g\<y}$ as desired.

%% This makes it easy to incrementalize function application, $f\<x$; given
%% changes $\changes \df f g$ and $\changes \dx x y$ to the function and its
%% argument, we want to compute the change that takes us to the updated
%% application $g\<y$. By inverting \textsc{FnChange} we know that
%% $\changes{\df\<x\<\dx}{f\<x}{g\<y}$, so $\df\<x\<\dx$ gives us the desired
%% change.

%% If instead changes were given pointwise, letting $\D(A \to B)= \iso A \to \D B$,
%% then it'd be natural to let $\changes{\df}{f}{g} \iff \fa{x}
%% \changes{\df\<x}{f\<x}{g\<x}$.

Note also the mixture of monotonicity and non-monotonicity in the type $\iso A
\to \D A \to \D B$. Since our functions are monotone --- increasing inputs yield
increasing outputs --- function \emph{changes} are also monotone on input
changes $\D A$ --- a larger increase in the input yields a larger increase in
the output. However, there's no reason to expect the change in the output to
grow as the \emph{base point} increases --- hence the use of $\iso$.


\subsection{Zero-changes, derivatives, and faster fixed points}
\label{sec:derivatives}

If $\changesat A \dx x x$, we call $\dx$ a \emph{zero-change} to $x$. Usually
zero-changes are rather boring --- for example, a zero change to a set $x :
\tseteq{A}$ is any $\dx \subseteq x$, and so $\emptyset$ is always a zero
change.
%
However, there is one very interesting exception: function zero changes. Suppose
$\changesat{A \to B}{\df}{f}{f}$. This implies that

\begin{equation}\label{eqn:derivative}
  \changesat A \dx x y \implies \changesat B{\df\<x\<\dx}{f\<x}{f\<y}
\end{equation}

\noindent
In other words, $\df$ yields the change in the output of $f$ given a change to
its input.
%
This is exactly the property of $\name{step}'$ that made it useful for
semi\naive\ evaluation --- indeed, $\name{step}'$ is a zero-change to
\name{step}, modulo not taking the base point $x$ as an argument:

\[ \changesat{\tseteq A} \dx x y \implies
\changesat{\tseteq A}{\name{step}'\<\dx}{\name{step}\<x}{\name{step}\<y}
\]

\noindent
Function zero changes are so important we give them a special name:
\emph{derivatives}. We now have enough machinery to prove correct a
general \emph{semi\naive\ fixed point strategy}. First, observe that:

\begin{lemma}\label{lem:DeltaL}
  At every semilattice type $L$, we have $\D L = L$ and
  $\changesat{L}{\dx}{x}{y} \iff (x \binvee \dx) = y$.
\end{lemma}

%% \begin{proof}
%%   By induction on semilattice types $L$.
%% \end{proof}

\noindent
This holds by a simple induction on semilattice types $L$. Now, given
a monotone map $f : L \to L$ and its derivative $f' : \iso L \to L \to
L$, we can find $f$'s fixed-point as the limit of the sequence $x_i$
defined:

\begin{align*}
  x_0 &= \bot & x_{i+1} &= x_i \vee \dx_i\\
  \dx_0 &= f\<\bot & \dx_{i+1} &= f'\<x_i\<\dx_i
\end{align*}

\noindent Let $\fastfix\<(f,f') = \bigvee_i x_i$. By induction and the
derivative property, we have $\changes{\dx_{i+1}}{x_i}{f\<x_i}$ and so
$x_i = f^i\<x$, and therefore $\fastfix\<(f,f') = \efix f$. Moreover,
if $L$ has no infinite ascending chains, we will reach our fixed point
$x_i = x_{i+1}$ in a finite number of iterations.

\label{sec:seminaive-strategy}

This leads directly to our strategy for semi\naive\ Datafun. \Citet{incremental}
defines a static transformation $\Deriv e$ which computes the change in $e$
given the change in its free variables; it \emph{incrementalizes} $e$. Our goal
is not to incrementalize Datafun \emph{per se}, but to find fixed points faster.
Consequently, we define two mutually recursive transformations: $\phi e$, which
computes $e$ faster by replacing fixed points with calls to \fastfix; and
$\delta e$, which incrementalizes $\phi e$ so that we can compute the derivative
of fixed point functions.
%
In order to define $\phi$ and $\delta$ and show them correct, however, we first
need a fuller account of Datafun's type system and semantics.
