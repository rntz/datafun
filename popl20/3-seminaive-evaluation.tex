\section{From Semi\naive{} Evaluation to the Incremental \boldfn-Calculus}
\label{sec:seminaive-and-ilc}

Consider the following Datalog fragment:\footnote{This example and explanation
  of semi\naive\ evaluation is borrowed almost entirely from
  \citet{DBLP:conf/esop/Alvarez-Picallo19}.}

\begin{align*}
  \name{path}(x,y) &\gets \name{edge}(x,y)
  &
  \name{path}(x,y) &\gets \name{edge}(x,y) \wedge \name{path}(y,z)
\end{align*}

\noindent
The denotation of \name{path} is the least fixed point of the rules defining
it.
% mention Kleene fixed point theorem?
We can compute this by repeatedly applying these rules until the collection
of known paths (initially empty) stops growing.
%
For example, if the \name{edge} relation is \{(1,\,2), (2,\,3), (3,\,4)\}, we
get the following evaluation trace:

%% TODO: not enough vertical space around this
\begin{center}
  \setlength\tabcolsep{1em}
  \def\arraystretch{1.1}
  \begin{tabular}{@{}rll@{}}
    Step
    & Previously known paths
    & Paths deduced at this step
    \\\midrule
    0
    & none
    & (1,\,2) (2,\,3) (3,\,4)
    \\
    1
    & (1,\,2) (2,\,3) (3,\,4)
    & (1,\,2) (2,\,3) (3,\,4) (1,\,3) (2,\,4)
    \\
    2
    & (1,\,2) (2,\,3) (3,\,4) (1,\,3) (2,\,4)
    & (1,\,2) (2,\,3) (3,\,4) (1,\,3) (2,\,4) (1,\,4)
    \\
    3
    & (1,\,2) (2,\,3) (3,\,4) (1,\,3) (2,\,4) (1,\,4)
    & as above
  \end{tabular}
\end{center}

\noindent We have now reached the desired fixed point. However, this process is
quite wasteful: we deduced the path (1,\,2) at \emph{every} iteration; ideally
we'd only deduce it once. On a chain of $n$ edges, we deduce $\Theta(n^3)$
facts, even though there are only $\Theta(n^2)$ paths!

The standard improvement to this strategy is known as
\emph{semi\naive\ evaluation}~\cite{seminaive}, which transforms recursive rules
into explicitly iterative time-indexed rules of two kinds: \emph{delta} rules,
to compute the new facts at each iteration; and \emph{accumulator} rules, to
collect these facts into a final result.
%
%% \begin{itemize}
%% \item \emph{Delta} rules, which compute the \emph{new} facts at each iteration.
%% \item \emph{Accumulator} rules, which accumulate these to compute the final result.
%% \end{itemize}
%
\noindent In this case, the delta rule is simple: we only discover new paths at
iteration $i+1$ by appending edges to paths which were new at iteration $i$:

\begin{align*}
  \name{path}_{i+1}(x,y) &\gets \name{path}_i(x,y)
  &
  \D\name{path}_0(x,y) &\gets \name{edge}(x,y)
  \\
  \name{path}_{i+1}(x,y) &\gets \D\name{path}_i(x,y)
  &
  \D\name{path}_{i+1}(x,z) &\gets \name{edge}(x,y) \wedge \D\name{path}_i(y,z)
\end{align*}

\noindent This yields the execution trace:

\begin{center}
  \setlength\tabcolsep{1em}
  \def\arraystretch{1.1}
  \begin{tabular}{@{}rll@{}}
    Step & $\name{path}_i$ & $\D\name{path}_i$
    \\\midrule
    0 & empty & (1,\,2) (2,\,3) (3,\,4)
    \\
    1 & (1,\,2) (2,\,3) (3,\,4) & (1,\,3) (2,\,4)
    \\
    2 & (1,\,2) (2,\,3) (3,\,4) (1,\,3) (2,\,4) & (1,\,4)
    \\
    3 & (1,\,2) (2,\,3) (3,\,4) (1,\,3) (2,\,4) (1,\,4) & empty
  \end{tabular}
\end{center}

%% TODO: email Alvarez-Picallo, Michael P-J, etc; they say "a quadratic
%% computation into a linear one" but AFAICT this is not so! Unless the claim is
%% that the naive version is quadratic in the _output_?!
%%
%% Perhaps we should say, "--- our computation is now linear in the size of its
%% output". But is this really justified?
This is much better --- we have turned a cubic computation into a quadratic one.
%
The semi\naive\ transformation is a kind of \emph{incremental computation}: at
each stage we compute the changes in the rule given the previous changes to its
inputs.
%
\todo{more?}


\subsection{Finding Fixed Points Faster via Derivatives}

Now let's move from Datalog to Datafun. The transitive closure of \name{edge} is
the fixed point of the monotone function \name{step} defined by:

\nopagebreak[2]
\[\begin{array}{l}
  \name{step} \<\name{path} = \name{edge} \cup
  \setfor{(x,z)}{(x,y) \in \name{edge}, (y,z) \in \name{path}}
\end{array}\]

\noindent
The \naive\ way to compute \name{step}'s fixed point is to iterate it: start
with \(\name{path}_0 = \emptyset\) and compute \(\name{path}_{i+1} =
\name{step}\<\name{path}_i\) until \(\name{path}_i = \name{path}_{i+1}\).
%
But since $\name{path}_i \subseteq \name{step}\<\name{path}_i$, each iteration
re-computes every path found by the previous iteration.
%
Following Datalog, we'd prefer to compute only the \emph{change} between
iterations.
%
So consider $\name{step}'$ defined by:

\nopagebreak[2]
\[\begin{array}{l}
  \name{step}' \<\name{dpath} =
  \setfor{(x,z)}{(x,y) \in \name{edge}, (y,z) \in \name{dpath}}
\end{array}\]

\noindent
Observe that $\name{step} \<(\name{path} \cup \name{dpath}) =
\name{step}\<\name{path} \cup \name{step}'\<\name{dpath}$.
%
That is, $\name{step}'$ tells us how \name{step} changes as its input grows.
%
Using this property, we can directly compute the changes $\name{dpath}_i$
between our iterations $\name{path}_i$:

\begin{align*}
  \name{dpath}_0 &= \name{step}\<\emptyset
  & \name{dpath}_{i+1} &= \name{step}'\<\name{dpath}_i
  & \name{path}_{i+1} &= \name{path}_i \cup \name{dpath}_i
\end{align*}
%
\todo{Explain why computing $\name{step}'\<\name{dpath}_i$ isn't
  \naive/ever-increasing.}
%
This is the analogue of semi\naive\ evaluation in a functional setting.

How do we find functions like $\name{step}'$?
%
\todo{explain how $\name{step}'$ looks like a derivative of $\name{step}$ in the
  sense of \citet{incremental}.}


%% Now let's move from Datalog to Datafun. The transitive closure of \name{edge} is
%% the fixed point of the monotone function $f$ defined by:

%% \nopagebreak[2]
%% \[\begin{array}{l}
%%   f \<\name{path} = \name{edge} \cup
%%   \setfor{(x,z)}{(x,y) \in \name{edge}, (y,z) \in \name{path}}
%% \end{array}\]

%% \noindent
%% The \naive{} way to find a fixed point is to iterate $f$, computing the
%% recurrence

%% \nopagebreak[2]
%% \begin{align*}
%%   x_0 &= \emptyset & x_{i+1} &= f\<x
%% \end{align*}

%% \noindent
%% However, since inductively $x_i \subseteq f\<x_i$, each iteration re-computes
%% every value found in the previous iteration.
%% %
%% Following Datalog, we'd prefer to compute only the \emph{change} between
%% iterations.
%% %
%% Since $x_i \subseteq x_{i+1}$, iterations change only by gaining elements. So
%% given a set $x$, and a set $\dx$ of added elements, let's imagine a function
%% $f'\<x\<\dx$ that tells us \emph{how $f\<x$ changes}, such that:

%% \begin{equation} f \<(x \cup \dx) = f\<x \cup f'\<x\<\dx
%%   \label{eqn:f-derivative}
%% \end{equation}

%% \noindent
%% Given such an $f'$ we can set up an equivalent recurrence:

%% \begin{align*}
%%   x_0 &= \emptyset & x_{i+1} &= x_i \cup \dx_i
%%   \\
%%   \dx_0 &= f\<\emptyset & \dx_{i+1} &= f' \<x_i \<\dx_i
%% \end{align*}

%% \noindent
%% From \cref{eqn:f-derivative} it follows inductively that $x_i = f^i\<\emptyset$.

%% So, how do we find such an $f'$? One trivial answer is to let $f'\<x\<\dx =
%% f\<(x \cup \dx)$. Unfortunately, this solution is worse than the problem:
%% re-computing $f$ is exactly what we wish to avoid!
%% %
%% For our \emph{specific} $f$, there is a better answer:

%% \nopagebreak[2]
%% \[\begin{array}{l}
%%   f' \<\name{path} \<\name{dpath} =
%%   \setfor{(x,z)}{(x,y) \in \name{edge}, (y,z) \in \name{dpath}}
%% \end{array}\]

