\section{From Semi\naive{} Evaluation to the Incremental \boldfn-Calculus}
\label{sec:seminaive-and-ilc}

Consider the following Datalog fragment:\footnote{This example and explanation
  of semi\naive\ evaluation is borrowed almost entirely from
  \citet{DBLP:conf/esop/Alvarez-Picallo19}.}

\begin{align*}
  \name{path}(x,y) &\gets \name{edge}(x,y)
  &
  \name{path}(x,z) &\gets \name{edge}(x,y) \wedge \name{path}(y,z)
\end{align*}


\noindent
The denotation of \name{path} is the least fixed point of the rules defining
it.
% mention Kleene fixed point theorem?
We can compute this by repeatedly applying these rules until the collection
of known paths (initially empty) stops growing.
%
For example, if the \name{edge} relation is \{(1,\,2), (2,\,3), (3,\,4)\}, we
get the following evaluation trace:

%% TODO: make sure there's enough vertical space around this.
\begin{center}
  \setlength\tabcolsep{1em}
  \begin{tabular}{@{}rll@{}}
    Step
    & Previously known paths
    & Paths deduced at this step
    \\\midrule
    0
    & none
    & (1,\,2) (2,\,3) (3,\,4)
    \\
    1
    & (1,\,2) (2,\,3) (3,\,4)
    & (1,\,2) (2,\,3) (3,\,4) (1,\,3) (2,\,4)
    \\
    2
    & (1,\,2) (2,\,3) (3,\,4) (1,\,3) (2,\,4)
    & (1,\,2) (2,\,3) (3,\,4) (1,\,3) (2,\,4) (1,\,4)
    \\
    3
    & (1,\,2) (2,\,3) (3,\,4) (1,\,3) (2,\,4) (1,\,4)
    & as above
  \end{tabular}
\end{center}

\noindent We have now reached the desired fixed point. However, this process is
quite wasteful: we deduced the path (1,\,2) at \emph{every} iteration; ideally
we'd only deduce it once. On a chain of $n$ edges, we deduce $\Theta(n^3)$
facts, even though there are only $\Theta(n^2)$ paths!

The standard improvement to this strategy is
\emph{semi\naive\ evaluation}~\cite{seminaive}, which transforms recursive rules
into explicitly iterative time-indexed rules of two kinds: \emph{derivative}
rules, to compute the new facts at each iteration; and \emph{accumulator} rules,
to collect these facts into a final result.
%
In this case, the derivative rule is simple: we discover new paths at iteration
$i+1$ by appending edges to paths which were new at iteration $i$:

\begin{align*}
  \name{dpath}_0(x,y) &\gets \name{edge}(x,y)
  \\
  \name{dpath}_{i+1}(x,z) &\gets \name{edge}(x,y) \wedge \name{dpath}_i(y,z)
  \\
  \name{path}_{i+1}(x,y) &\gets \name{path}_i(x,y) \vee \name{dpath}_i(x,y)
\end{align*}

\noindent This yields the execution trace:

\begin{center}
  \setlength\tabcolsep{1em}
  \begin{tabular}{@{}rll@{}}
    Step & $\name{path}_i$ & $\name{dpath}_i$
    \\\midrule
    0 & empty & (1,\,2) (2,\,3) (3,\,4)
    \\
    1 & (1,\,2) (2,\,3) (3,\,4) & (1,\,3) (2,\,4)
    \\
    2 & (1,\,2) (2,\,3) (3,\,4) (1,\,3) (2,\,4) & (1,\,4)
    \\
    3 & (1,\,2) (2,\,3) (3,\,4) (1,\,3) (2,\,4) (1,\,4) & empty
  \end{tabular}
\end{center}

%% TODO: email Alvarez-Picallo, Michael P-J, etc; they say "a quadratic
%% computation into a linear one" but AFAICT this is not so! Unless the claim is
%% that the naive version is quadratic in the _output_?!
%%
%% Perhaps we should say, "--- our computation is now linear in the size of its
%% output". But is this really justified?
\noindent
This is much better --- we have turned a cubic computation into a quadratic one!


\subsection{Semi\naive\ evaluation as incremental computation}

Now let's move from Datalog to Datafun. The transitive closure of \name{edge} is
the fixed point of the monotone function \name{step} defined by:

\[
\name{step} \<\name{path} = \name{edge} \cup
\setfor{(x,z)}{(x,y) \in \name{edge}, (y,z) \in \name{path}}
\]

\noindent
The \naive\ way to compute \name{step}'s fixed point is to iterate it: start
with \(\name{path}_0 = \emptyset\) and compute \(\name{path}_{i+1} =
\name{step}\<\name{path}_i\) for increasing $i$ until \(\name{path}_i =
\name{path}_{i+1}\).
%
But since $\name{path}_i \subseteq \name{step}\<\name{path}_i$, each iteration
re-computes every path found by the previous iteration.
%
Following Datalog, we'd prefer to compute only the \emph{change} between
iterations.
%
So consider $\name{step}'$ defined by:

\[
\name{step}' \<\name{dpath} =
\setfor{(x,z)}{(x,y) \in \name{edge}, (y,z) \in \name{dpath}}
\]

\noindent
Observe that $\name{step} \<(\name{path} \cup \name{dpath}) =
\name{step}\<\name{path} \cup \name{step}'\<\name{dpath}$.
%
That is, $\name{step}'$ tells us how \name{step} changes as its input grows.
%
Using this property, we can directly compute the changes $\name{dpath}_i$
between our iterations $\name{path}_i$:

%% \begin{align*}
%%   \name{dpath}_0 &= \name{step}\<\emptyset
%%   & \name{dpath}_{i+1} &= \name{step}'\<\name{dpath}_i
%%   & \name{path}_{i+1} &= \name{path}_i \cup \name{dpath}_i
%% \end{align*}

%% \[\def\arraystretch{1.2}
%% \begin{array}{rclcl}
%%   \name{dpath}_0 &=& \name{step}\<\emptyset
%%   &=& \name{edge}
%%   \\
%%   \name{dpath}_{i+1} &=& \name{step}'\<\name{dpath}_i
%%   &=& \setfor{(x,z)}{(x,y) \in \name{edge}, (y,z) \in \name{dpath}_i}
%%   \\
%%   \name{path}_{i+1} &=& \name{path}_i \cup \name{dpath}_i
%% \end{array}\]

\begin{align*}
  \name{dpath}_0
  &= \name{step}\<\emptyset
  = \name{edge}
  \\
  \name{dpath}_{i+1}
  &= \name{step}'\<\name{dpath}_i
  = \setfor{(x,z)}{(x,y) \in \name{edge}, (y,z) \in \name{dpath}_i}
  \\
  \name{path}_{i+1}
  &= \name{path}_i \cup \name{dpath}_i
\end{align*}

\noindent These exactly mirror the derivative and accumulator rules for
\(\name{path}_i\) and \(\name{dpath}_i\) we gave earlier.
%
\todo{Explain how this lets us compute $\name{path}_i$ more
  efficiently and wait until it quiesces as before.}

The problem of semi\naive\ evaluation for Datafun, then, reduces to the problem
of finding functions like $\name{step}'$, which compute the change in a
function's output given a change to its input.
%
This is a problem of \emph{incremental computation}, and since Datafun is a
functional language, we turn to the
\emph{incremental \fn-calculus}~\citep{incremental}.


\subsection{Change structures and the incremental \boldfn-calculus}
\label{sec:seminaive-via-incremental}

To formalize the notion of change, an incremental \fn-calculus associates every
type $A$ with a \emph{change structure}, consisting of:\footnote{Our notion of
  change structure differs significantly from that of \citeauthor{incremental};
  we discuss the divergence in \cref{sec:differences-from-incremental}. As we do
  not use change structures \emph{per se} in our proofs, we treat them
  informally, as a source of intuition rather than rigor.}

\begin{enumerate}
\item A type $\D A$ of possible changes to values of type $A$.
\item A relation $\changesat{A}{\dx}{x}{y}$ for $\dx : \D A$ and $x,y : A$,
  glossed as ``$\dx$ changes $x$ into $y$''.
\end{enumerate}

\noindent
Since the iterations of a fixed point grow monotonically, in Datafun we only
need \emph{increasing} changes.
%
For example, sets change by gaining new elements:

\begin{align*}
  \D\tseteq{A} &= \tseteq{A}
  &
  \changesat{\tseteq{A}}{\dx}{x}{x \cup \dx}
\end{align*}

\noindent Set changes may be the most significant for fixed point purposes, but
to handle all of Datafun we need a change structure for every type. For products
and sums, for example, the change structure is pointwise:

\begin{align*}
  \D\tunit &= \tunit
  &
  \D(A \x B) &= \D A \x \D B
  &
  \D(A + B) &= \D A + \D B
\end{align*}

\begin{align*}
  \changesat{\tunit}{\tuple{}}{\tuple{}}{\tuple{}}
  &&
  \infer{
    \fa{i} \changesat{A_i}{\dx_i}{x_i}{y_i}
  }{\textstyle\changesat{A_1 \x A_2}
    {\tuple{\vec\dx}}
    {\tuple{\vec x}}
    {\tuple{\vec y}}
  }
  %
  %% \infer{
  %%   \fa{i} \changesat{A_i}{\dx_i}{x_i}{y_i}
  %% }{\textstyle\changesat{A_1 \x A_2}
  %%   {\tuple{\dx_1,\dx_2}}
  %%   {\tuple{x_1,x_2}}
  %%   {\tuple{y_1,y_2}}
  %% }
  %
  %% \infer{
  %%   \changesat{A}{\da}{a_1}{a_2}
  %%   \\
  %%   \changesat{B}{\db}{b_1}{b_2}
  %% }{\textstyle\changesat{A \x B}
  %%   {\tuple{\da,\db}}
  %%   {\tuple{a_1,b_1}}
  %%   {\tuple{a_2,b_2}}
  %% }
  &&
  \infer{
    \changesat{A_i}{\dx_i}{x_i}{y_i}
  }{
    \textstyle\changesat{A_1 + A_2}{\inj i \dx}{\inj i x}{\inj i y}
  }
\end{align*}
\vspace{0pt} % FIXME. yes, this makes a difference, believe it or not

%% \begin{align*}
%%   \D\tunit &= \tunit
%%   &
%%   \D(A \x B) &= \D A \x \D B
%%   &
%%   %% \infer{
%%   %%   %\fa{i} \changesat{A_i}{\dx_i}{x_i}{y_i}
%%   %%   \changesat{A_1}{\dx_1}{x_1}{y_1}
%%   %%   \\
%%   %%   \changesat{A_2}{\dx_2}{x_2}{y_2}
%%   %% }{\changesat{A_1 \x A_2}
%%   %%   {\tuple{\dx_1,\dx_2}}
%%   %%   {\tuple{x_1,x_2}}{\tuple{y_1,y_2}}
%%   %% }
%%   %
%%   %% \infer{
%%   %%   \fa{i} \changesat{A_i}{\dx_i}{x_i}{y_i}
%%   %% }{\changesat{A_1 \x A_2}
%%   %%   {\tuple{\dx_1,\dx_2}}
%%   %%   {\tuple{x_1,x_2}}{\tuple{y_1,y_2}}
%%   %% }
%%   %
%%   \infer{
%%     \fa{i} \changesat{A_i}{\dx_i}{x_i}{y_i}
%%   }{\textstyle\changesat{\prod_i A_i}
%%     {\tuple{\vec\dx}}
%%     {\tuple{\vec x}}{\tuple{\vec y}}
%%   }
%% \end{align*}

%% %\noindent For sums, the change structure is disjoint:

%% \begin{align*}
%%   \D(A + B) &= \D A + \D B
%%   &
%%   \infer{
%%     \changesat{A_i}{\dx_i}{x_i}{y_i}
%%   }{
%%     \textstyle\changesat{A_1 + A_2}{\inj i \dx}{\inj i x}{\inj i y}
%%   }
%% \end{align*}

\noindent
Since we only consider increasing changes, and $\iso A$ is ordered discretely,
the only ``change'' permitted is to stay the same. Consequently, no information
is necessary to indicate what changed:

\begin{align*}
  \D(\iso A) &= \tunit
  &&
  \changesat{\iso A}{\tuple{}}{x}{x}
\end{align*}

\noindent
Finally we come to the most interesting case: functions.

\begin{align*}
  \D(A \to B) &= \iso A \to \D A \to \D B
  &
  \infer[FnChange]{
    \fa{\changesat A \dx x y}
    \changesat B {\df\<x\<\dx} {f\<x} {g\<y}
  }{
    \changesat{A \to B}{\df}{f}{g}
  }
\end{align*}

\noindent
Observe that a function change $\df : \iso A \to \D A \to \D B$ takes two
arguments: a base point $x$ and a change $\dx$.
%
To understand why we need both, consider incrementalizing function application:
we wish to know how $(f\<x)$ changes as both $f$ and $x$ change. So fix
$\changes{\df}{f}{g}$ and $\changes{\dx}{x}{y}$. How do we find the change $f\<x
\changesto g\<y$ that updates both function and argument? If changes were given
pointwise, taking only a base point, we'd stipulate that $\changes{\df}{f} g$
iff $\fa{x} \changes{\df\<x}{f\<x}{g\<x}$. But this only gets us to $g\<x$, not
$g\<y$: we've accounted for the change in the function, but not the argument.
%
We can account for both by giving $\df$ an additional parameter: not just the
base point $x$ but also the change $\dx$ to it. Then by inverting
\textsc{FnChange} we have $\changes{\df\<x\<\dx}{f\<x}{g\<y}$ as desired.

%% This makes it easy to incrementalize function application, $f\<x$; given
%% changes $\changes \df f g$ and $\changes \dx x y$ to the function and its
%% argument, we want to compute the change that takes us to the updated
%% application $g\<y$. By inverting \textsc{FnChange} we know that
%% $\changes{\df\<x\<\dx}{f\<x}{g\<y}$, so $\df\<x\<\dx$ gives us the desired
%% change.

%% If instead changes were given pointwise, letting $\D(A \to B)= \iso A \to \D B$,
%% then it'd be natural to let $\changes{\df}{f}{g} \iff \fa{x}
%% \changes{\df\<x}{f\<x}{g\<x}$.

Second, note the mixture of monotonicity and non-monotonicity in $\df : \iso A
\to \D A \to \D B$. Since our functions are monotone --- increasing inputs yield
increasing outputs --- function \emph{changes} are also monotone on input
changes $\D A$ --- a larger increase in the input yields a larger increase in
the output. However, there's no particular reason to expect the change in the
output to increase as the \emph{base point} increases --- hence the $\iso A$.

\todo{Explain:
  \begin{itemize}
  \item Function derivatives are zero-changes.

  \item Since we're tracking changes during fixed point iteration, and these
    increase monotonically, we only allow increasing changes. So $\iso A$
    (discussed in \cref{sec:datalog-and-datafun}) prevents things from changing!

  \item How $\name{step}'$ looks like a derivative of $\name{step}$ in the
  sense of \citet{incremental}.
  \end{itemize}

  Do \textbf{not} explain:
  \begin{itemize}
  \item what it does to contexts or variables
  \item derivatives of expressions
  \end{itemize}
}

%% Revisiting our running example, the crucial property of $\name{step}'$ now
%% becomes:

%% \[ \changesat{\tseteq{A}}{\name{step}' \<\dx}{\name{step}\<x}{\name{step}\<(x
%%   \cup \dx)} \]

%% or, more suggestively:

%% \[ \changesat{\tseteq A}{\dx}{x}{y} \implies
%% \changesat{\tseteq A}{\name{step}' \<\dx}{\name{step}\<x}{\name{step}\<y}
%% \]


\subsection{Examples of semi\naive\ Datafun programs}
\XXX

In order to explain in detail how our static transformation operates and why it
is correct, however, we first need a fuller account of Datafun's type system and
semantics.
