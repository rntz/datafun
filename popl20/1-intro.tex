\section{Introduction}
\label{sec:intro}

Datalog~\cite{datalog}, along with the $\pi$-calculus and
\fn-calculus, is one of the jewel languages of theoretical computer
science, connecting programming language theory, database theory, and
complexity theory. In terms of programming languages, Datalog can be
understood as a fully declarative subset of Prolog which is guaranteed
to terminate and so can be evaluated in both top-down and bottom-up
fashion. In terms of database theory, it is equivalent to the
extension of relational algebra with a fixed point operator, and in
terms of complexity theory, it does not just terminate, but also (when
full Datalog with stratified negation is interpreted over ordered
structures) characterizes polytime
computation~\cite{datalog-polytime}.

In addition to its theoretical elegance, over the past twenty years
Datalog has seen a surprisingly wide array of uses across a variety of
practical domains, both in research and in industry.

Whaley and Lam \cite{whaley-lam,whaley-phd} implemented pointer
analysis algorithms in Datalog, and found that they could reduce their
analyses from thousands of lines of C code to \emph{tens} of lines of
Datalog code, while retaining competitive performance. The DOOP
pointer analysis framework~\cite{doop}, built using the Souffl\'{e}
Datalog engine~\cite{souffle}, shows that this approach can handle
almost all of industrial languages like Java, even analysing features
like reflection~\cite{doop-java-reflection}. Semmle has developed the
.QL language~\cite{semmlecode,ql-inference} based on Datalog for
analysing source code (which was used to analyze the code for NASA's
Curiosity Mars rover), and LogicBlox has developed the
LogiQL~\cite{logicblox} language for business analytics and retail
prediction. The Boom project at Berkeley has developed the Bloom
language for distributed programming~\cite{bloom}, and the Datomic
cloud database~\cite{datomic} uses Datalog (embedded in Clojure) as
its query language. Microsoft's SecPAL language~\cite{secpal} uses
Datalog as the foundation of its decentralised authorization
specification language. In each case, when the problem formulated in
Datalog, the specification became directly implementable, while
remaining dramatically shorter and clearer than alternatives
implemented in more conventional languages.

% \todo{We should cite Yannis's
%   recent SNAPL paper ``Next-Paradigm Programming Languages: What Will They
%   Look Like and What Changes Will They Bring?''}

However, there are two flies in the ointment. First, even though each
of these applications is supported by the skeleton of Datalog, they
all had to extend it significantly beyond the theoretical core
calculus.  For example, core Datalog does not even support arithmetic,
since its semantics only speaks of finite sets supporting equality of
their elements. Moreover, arithmetic is not a trivial extension, since
it can greatly complicates the semantics (for example, proving that
termination continues to hold). So despite the fact that kernel
Datalog has a very clean semantics, its metatheory seemingly needs to
be laboriously re-established once again for each extension.

A natural approach to solving this problem is to find a language in
which to write the extensions, which preserves the semantic guarantees
that Datalog offers. Two such proposals are the Flix
language~\cite{flix} and the Datafun language~\cite{datafun}. 
Conveniently for our exposition, these two languages embody two
alternative design strategies.

%% FIXME: this is not up-to-date with Flix's current design. I think
%% it's even not up-to-date with Flix's old design. Old Flix wasn't a
%% functional program building a Datalog program, it's the reverse: a
%% Datalog program that can run functional code in certain places to
%% eg. compute lattice aggregations.
%
%% New Flix has both functional code generating Datalog code, and
%% Datalog code being able to run functional code in certain places.
%% So it seems like it should be _very_ close to Datafun in
%% expressivity. I think Madsen has either a draft or a
%% soon-to-be-published paper on this new design.
%
%% Differences between Flix & Datafun as I see them:
%%
%% - Flix doesn't have monotonicity types.
%
%% - Instead Flix integrates with SMT solvers for lightweight
%%   verification of properties including monotonicity,
%%   distributivity, soundness, completeness, etc. However, mostly
%%   only works for 1st-order stuff.
%
%% - Flix makes it easy to extend language with new semilattice types:
%%   you just write the code for the join operation.
%
%% - In Datafun, Datalog is just a mode of use of functional language.
%%   Flix separates the relational/Datalog and functional
%%   sublanguages.
%
%% - In Flix, the semantics of passing around Datalog code in the
%%   functional language are unclear; talking with Madsen, he seemed
%%   unsure what design choice was best.
%
%% But a summary this^ long belongs in related work, not here.
Flix adopts the route of treating Datalog as an embedded
domain-specific language~\cite{edsl}. That is, Flix is a fairly
conventional (albeit well-designed) functional programming language
roughly comparable to ML or Haskell, extended with types representing
Datalog predicates and program. The evaluation of a Flix program
builds a Datalog program, which is then handed off to a custom Datalog
engine (albeit extended to support arbitrary semilattices).  This
stratification means that (a) standard Datalog implementation
techniques can be used mostly off-the-shelf, but that (b) its
functional programming side is mostly a very powerful macro system for
Datalog.  The only way Flix code runs at Datalog execution time is via
the definition of new semilattices (which are regular functional
language code implementing a semilattice interface), and for this Flix
offers program-verification style correctness checking (including
SMT-based tooling).

Datafun takes a somewhat different approach. Like Flix, it is a
functional programming language, but unlike Flix, its type discipline
has been extended to support tracking whether functions are monotone
or not. Then, Datalog-style recursively defined relations are just
ordinary recursive defininitions on set-valued functions, with
monotonicity typing ensuring that every definable function on
relations continues to make sense in the model-theoretic semantics of
Datalog. Tracking monotonicity information in types permits a much
tighter integration between the functional and logic programming
styles, but it comes at the cost that many of Datalog's standard
implementation techniques, which were originally developed in the
context of a first-order logic language, are no longer obviously
applicable in the higher-order functional setting.

Second, actually
making Datalog perform well enough to use in practice calls for very
sophisticated program analysis and compiler engineering. (This is a
familiar experience from the database community, where query planners
must encode a startling amount of knowledge to optimize relatively
simple SQL queries.) A wide variety of techniques for optimizing
Datalog programs have been studied in the literature, such as using
binary decision diagrams to represent relations~\cite{whaley-phd},
exploiting the structure of well-behaved subsets (e.g.,
CFL-reachability problems correspond to the ``chain program'' fragment
of Datalog~\cite{chain-programs}), and combining top-down and
bottom-up evaluation via the ``magic sets''
algorithm~\cite{magic-sets}.

Today, one of the workhorse techniques for implementing bottom-up
Datalog engines is \emph{semi\naive\
  evaluation}~\cite{seminaive}. This optimization improves the
performance of Datalog's most distinctive feature: recursively defined
predicates. Such a predicate can be understood as the fixed point of a
set-valued function $f$; a \naive\ fixed point computation will
directly compute the limit of the sequence
$\emptyset, f(\emptyset), f^2(\emptyset), \dots$. At each iteration,
many values are unnecessarily re-computed; semi\naive\ evaluation
computes a safe approximation of the \emph{new} values generated at
each step. This optimization is critical, as it can
\emph{asymptotically} improve the performance of Datalog queries.


\paragraph{Contributions} The semi\naive\ evaluation algorithm is
defined partly as a program transformation on sets of Datalog rules,
and partly as a modification of the fixed point computation algorithm.
The central contribution of this paper is to give an extended version
of this transformation which works on higher-order programs written
in the Datafun language. 

\begin{itemize}
\item We reformulate Datafun in terms of a kernel calculus based on
  the modal logic S4. Instead of giving a calculus with distinct
  monotonic and discrete function types, as in the original Datafun
  paper, we make discreteness into a comonad. In addition to
  regularizing the calculus and slightly improving its expressiveness,
  the explicit comonadic structure lets us impose a modal constraint
  on recursion reminiscent of Hoffman's work on safe
  recursion~\cite{hofmann-safe-recursion}. This brings the semantics
  of Datafun more closely in line with Datalog's, and substantially
  simplifies the program transformation we present.
  
\item We define a program transformation to statically convert
  well-typed Datafun programs into an incrementalized version. The
  translation is a compositional type-and-syntax-directed
  transformation, which works uniformly at all types including
  function types. Our approach builds upon the \emph{change structure}
  methodology for static program incrementalization introduced by
  \citet{incremental}, extending it to support for sum types, set
  types, comonads, and (well-founded) fixed points.

\item We establish the correctness of our transformation using a novel
  logical relation which simultaneously defines the changes connecting
  old and updated programs, as well as the optimized version of both.
  The fundamental lemma then lets us show that our transformation
  is semantics-preserving, in the sense that any closed program term
  of predicate type has the same meaning after optimization. 
  
\item We then discuss an implementation of semi\naive\ Datafun, which
  works by compiling Datafun to Haskell, in both \naive\ and
  semi\naive\ form. This lets us both give examples of the translation
  on nontrivial terms, and empirically demonstrate the asymptotic
  speedups predicted by the theory. We additionally discuss the
  (surprisingly modest) set of program optimizations we found
  helpful for putting the 
  optimization into  practice. 
\end{itemize}


%% \begin{itemize}

%% \item We define a program transformation to statically convert well-typed
%%   Datafun programs (including ones that use higher-order functions) into an
%%   incrementalized version. We then use a logical relation to show that our
%%   incrementalization is correct, and so can be used to optimize Datafun
%%   programs.

%% \item We also implement our program transformation in a small compiler, and use
%%   this to show that we can automatically compile Datafun terms into optimized
%%   Haskell programs with the expected operational behaviour.

%% \end{itemize}


%%% Local Variables:
%%% TeX-master: "seminaive-datafun"
%%% End:
