\section{Implementation and optimization}
\label{sec:implementation}

We have implemented a compiler from a fragment of Datafun (omitting sum types) to Haskell, available at \url{https://github.com/rntz/datafun/tree/popl20/v4-fastfix}.
%\url{http://www.rntz.net/datafun/popl20/}. % TODO
%
We use Haskell's \texttt{Data.Set} to represent Datafun sets, and typeclasses to implement Datafun's notions of equality and semilattice types.
%
We do no query planning and implement no join algorithms; relational
joins, written in Datafun as nested \kw{for}-loops, are compiled into nested
loops.
%
Consequently our performance is worse than any real Datalog engine.
%
However, we do implement the $\phi$ translation, along with the following
optimizations:

\begin{enumerate}
\item Propagating $\bot$; for example, rewriting $(e \vee \bot) \rewrites e$ and
  $(\efor{x}{e} \bot) \rewrites \bot$.

\item Inserting $\bot$ in place of semilattice-valued zero changes (for example,
  changes to discrete variables $\delta \dvar x$). This makes $\bot$-propagation
  more effective.

\item Recognising complex zero change expressions; for example, $\delta e
  \<\ebox{\phi f} \<\delta f$ is a zero change if $\delta e$ and $\delta f$ are.
  This allows more zero changes to be replaced by $\bot$, especially in
  higher-order code such as our regular expression example.
\end{enumerate}

\input{fig-seminaive-vs-naive-graph}

\noindent
To test whether the $\phi$ translation can produce the asymptotic performance
gains we claim, we benchmark two example Datafun programs:

\begin{enumerate}
\item Finding the transitive closure of a linear graph using the \name{trans}
  function from \cref{sec:generic-transitive-closure}. We chose this example
  because, as discussed in \cref{sec:seminaive-and-ilc}, it has a well
  understood asymptotic speed-up under semi\naive\ evaluation. This means that
  if we've failed to capture the essence of semi\naive\ evaluation, it should be
  highly visible.

\item Finding all matches of the regular expression \texttt{/a*/} in the string
  $\texttt{a}^n$, using the regex combinators from \cref{sec:regex-combinators}.
  Finding all matches for \texttt{/a*/} amounts to finding the reflexive,
  transitive closure of the matches of \texttt{/a/}, and on $\texttt{a}^n$ these
  form a linear graph. Thus it is a close computational analogue of our first
  example, written in a higher-order style. We chose this example to test
  whether our extension of semi\naive\ evaluation properly handles Datafun's
  distinctive feature: higher-order programming.
\end{enumerate}

We compiled each program in three distinct ways: \emph{\naive{}}, without the
$\phi$ transform (but with $\bot$-propagation); \emph{semi\naive{} raw}, with
the $\phi$ transform but without further optimization; and \emph{semi\naive{}
  optimized}, with the $\phi$ transform followed by all three optimizations listed
previously. The results are shown in \cref{fig:seminaive-vs-naive-graph}.
%
The measured times are substantially similar for transitive closure and regex
search across all three optimization levels, suggesting that higher-order code
does not pose a particular problem for our optimizations. However, compared to
\emph{\naive}, the $\phi$ transform alone (\emph{semi\naive\ raw}) provides only
a small speed-up, roughly 10--20\%. Only when followed by other optimizations
(\emph{semi\naive\ optimized}\kern.75pt) does it provide the expected asymptotic
speedup.\footnote{We also tried following the $\phi$ transform with only
  $\bot$-propagation, dropping the other two optimisations. This produced
  essentially the same results as \emph{semi\naive\ optimized}, so we have omitted it
  from \cref{fig:seminaive-vs-naive-graph}. It is unclear whether inserting
  $\bot$ or recognizing complex zero changes are ever necessary to achieve an
  asymptotic speed-up.

  It's also worth addressing the asymptotic performance of
  \emph{semi\naive\ optimized}. On regex search, for example, doubling the string
  length from $160$ to $320$ produces a slowdown factor of $\frac{.401}{.054}
  \approx 7.42$! However, since there are quadratically many matches and we find
  all of them, the best possible runtime is $O(n^2)$. Moreover, our nested-loop
  joins are roughly a factor of $n$ slower than optimal, so we expect $O(n^3)$
  behavior or worse. This back-of-the-envelope estimation predicts a slowdown of
  $2^3 = 8$, reasonably close to $7.42$. Phew!}

We believe this is because both $\phi(\efor x e ...)$ and $\delta(\efor x e
...)$ produce loops that iterate over at least every $\dvar x \in \phi e$.
Consulting our logical relation at set type, we see that in this case $e$ and
$\phi e$ will be identical, and so the number of iterations never shrinks.
However, as demonstrated in \cref{sec:seminaive-examples}, if the body can be
simplified to $\bot$, then we can eliminate the loop entirely by rewriting
$(\efor x e \bot)$ to $\bot$, which allows for asymptotic improvement.

As in Datalog, we do not expect semi\naive\ evaluation to be useful on
\emph{all} recursive programs. Under \naive\ evaluation, each iteration towards
a fixed point is more expensive than the last, so as a rule of thumb,
semi\naive\ evaluation is more valuable the more iterations required.

%% TODO: Discuss when \& why inlining might be helpful.
%% We speculate that \emph{function inlining} would also be a helpful optimization.
